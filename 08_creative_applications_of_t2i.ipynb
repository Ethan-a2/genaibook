{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creative Applications of Text-To-Image Models\n",
    "\n",
    "This notebook is a supplementary material for the Creative Applications of Text-To-Image Models of the [Hands-On Generative AI with Transformers and Diffusion Models book](https://learning.oreilly.com/library/view/hands-on-generative-ai/9781098149239/). This notebook includes:\n",
    "\n",
    "* The code from the book\n",
    "* Additional examples\n",
    "* Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-to-Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "\n",
    "from genaibook.core import get_device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Load the pipeline\n",
    "img2img_pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the pipeline to the device\n",
    "# Alternatively, img2img_pipeline.set enable_model_cpu_offload()\n",
    "img2img_pipeline.set enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image, make_image_grid\n",
    "from genaibook.core import SampleURL\n",
    "\n",
    "# Load the image\n",
    "url = SampleURL.ToyAstronauts\n",
    "init_image = load_image(url)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "# Pass the prompt and the image through the pipeline\n",
    "image = img2img_pipeline(prompt, image=init_image, strength=0.5).images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLInpaintPipeline\n",
    "\n",
    "# Load the pipeline\n",
    "inpaint_pipeline = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(device)\n",
    "\n",
    "img_url = SampleURL.DogBenchImage\n",
    "mask_url = SampleURL.DogBenchMask\n",
    "\n",
    "init_image = load_image(img_url).convert(\"RGB\").resize((1024, 1024))\n",
    "mask_image = load_image(mask_url).convert(\"RGB\").resize((1024, 1024))\n",
    "\n",
    "# Pass images and prompt through the pipeline\n",
    "prompt = \"A majestic tiger sitting on a bench\"\n",
    "image = inpaint_pipeline(\n",
    "    prompt=prompt,\n",
    "    image=init_image,\n",
    "    mask_image=mask_image,\n",
    "    num_inference_steps=50,\n",
    "    strength=0.80,\n",
    "    width=init_image.size[0],\n",
    "    heigth=init_image.size[1],\n",
    ").images[0]\n",
    "\n",
    "make_image_grid([init_image, mask_image, image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Weighting and Image Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Weighting and Merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compel import Compel, ReturnedEmbeddingsType\n",
    "\n",
    "# Use the penultimate CLIP layer as it is more expressive\n",
    "embeddings_type = (\n",
    "    ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED\n",
    ")\n",
    "compel = Compel(\n",
    "    tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2],\n",
    "    text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2],\n",
    "    returned_embeddings_type=embeddings_type,\n",
    "    requires_pooled=[False, True],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "# Prepare the prompts\n",
    "prompts = []\n",
    "prompts.append(\"a humanoid robot eating pasta\")\n",
    "prompts.append(\n",
    "    \"a humanoid+++ robot eating pasta\"\n",
    ")  # make its humanoid characteristics a bit more pronounced\n",
    "prompts.append(\n",
    "    '[\"a humanoid robot eating pasta\", \"a van gogh painting\"].and(0.8, 0.2)'\n",
    ")  # make it van gogh!\n",
    "\n",
    "images = []\n",
    "for prompt in prompts:\n",
    "    # Use the same seed across generations\n",
    "    generator = torch.Generator(device=device).manual_seed(1)\n",
    "\n",
    "    # The compel library returns both the conditioning vectors \n",
    "    # and the pooled prompt embeds\n",
    "    conditioning, pooled = compel(prompt)\n",
    "\n",
    "    # We pass the conditioning and pooled prompt embeds to the pipeline\n",
    "    image = pipeline(\n",
    "        prompt_embeds=conditioning,\n",
    "        pooled_prompt_embeds=pooled,\n",
    "        num_inference_steps=30,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    images.append(image)\n",
    "make_image_grid(images, rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing Diffusion Images with Semantic Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import SemanticStableDiffusionPipeline\n",
    "\n",
    "semantic_pipeline = SemanticStableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16, variant=\"fp16\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=device).manual_seed(100)\n",
    "out = semantic_pipeline(\n",
    "    prompt=\"a photo of the face of a man\",\n",
    "    negative_prompt=\"low quality, deformed\",\n",
    "    generator=generator,\n",
    ")\n",
    "out.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=device).manual_seed(100)\n",
    "out = semantic_pipeline(\n",
    "    prompt=\"a photo of the face of a man\",\n",
    "    negative_prompt=\"low quality, deformed\",\n",
    "    editing_prompt=\"smiling, smile\",\n",
    "    edit_guidance_scale=4,\n",
    "    edit_warmup_steps=10,\n",
    "    edit_threshold=0.99,\n",
    "    edit_momentum_scale=0.3,\n",
    "    edit_mom_beta=0.6,\n",
    "    reverse_editing_direction=False,\n",
    "    generator=generator,\n",
    ")\n",
    "out.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=device).manual_seed(100)\n",
    "out = semantic_pipeline(\n",
    "    prompt=\"a photo of the face of a man\",\n",
    "    negative_prompt=\"low quality, deformed\",\n",
    "    editing_prompt=\"glasses, wearing glasses\",\n",
    "    reverse_editing_direction=False,\n",
    "    edit_warmup_steps=10,\n",
    "    edit_guidance_scale=4,\n",
    "    edit_threshold=0.99,\n",
    "    edit_momentum_scale=0.3,\n",
    "    edit_mom_beta=0.6,\n",
    "    generator=generator,\n",
    ")\n",
    "out.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=device).manual_seed(100)\n",
    "out = semantic_pipeline(\n",
    "    prompt=\"a photo of the face of a man\",\n",
    "    negative_prompt=\"low quality, deformed\",\n",
    "    editing_prompt=[\n",
    "        \"smiling, smile\",\n",
    "        \"glasses, wearing glasses\",\n",
    "    ],\n",
    "    reverse_editing_direction=[False, False],\n",
    "    edit_warmup_steps=[10, 10],\n",
    "    edit_guidance_scale=[6, 6],\n",
    "    edit_threshold=[0.99, 0.99],\n",
    "    edit_momentum_scale=0.3,\n",
    "    edit_mom_beta=0.6,\n",
    "    generator=generator,\n",
    ")\n",
    "out.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Image Editing via Inversion\n",
    "\n",
    "### Editing with LEDITS++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LEditsPPPipelineStableDiffusion\n",
    "\n",
    "# Load the model as usual\n",
    "pipe = LEditsPPPipelineStableDiffusion.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "pipe.to(device)\n",
    "\n",
    "image = load_image(SampleURL.ManInGlasses).convert(\"RGB\")\n",
    "\n",
    "# Invert the image, gradually adding noise to it so\n",
    "# it can be denoised with modified directions,\n",
    "# effectively providing an edit\n",
    "pipe.invert(image=image, num_inversion_steps=50, skip=0.2)\n",
    "\n",
    "# Edit the image with an editing prompt\n",
    "edited_image = pipe(\n",
    "    editing_prompt=[\"glasses\"],\n",
    "    # tell the model to remove the glasses by editing the direction\n",
    "    reverse_editing_direction=[True],\n",
    "    edit_guidance_scale=[1.5],\n",
    "    edit_threshold=[0.95],\n",
    ").images[0]\n",
    "\n",
    "make_image_grid([image, edited_image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Image Editing via Instruction Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import (\n",
    "    EDMEulerScheduler,\n",
    "    StableDiffusionXLInstructPix2PixPipeline,\n",
    ")\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "edit_file = hf_hub_download(\n",
    "    repo_id=\"stabilityai/cosxl\", filename=\"cosxl_edit.safetensors\"\n",
    ")\n",
    "\n",
    "# from_single_file loads a diffusion model from a single diffusers file\n",
    "pipe_edit = StableDiffusionXLInstructPix2PixPipeline.from_single_file(\n",
    "    edit_file, num_in_channels=8, is_cosxl_edit=True, torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# The model was trained so that the EDMEulerScheduler\n",
    "# is the correct noise scheduler for denoising\n",
    "pipe_edit.scheduler = EDMEulerScheduler(\n",
    "    sigma_min=0.002,\n",
    "    sigma_max=120.0,\n",
    "    sigma_data=1.0,\n",
    "    prediction_type=\"v_prediction\",\n",
    "    sigma_schedule=\"exponential\",\n",
    ")\n",
    "pipe_edit.to(device)\n",
    "\n",
    "prompt = \"make it a cloudy day\"\n",
    "image = load_image(SampleURL.Mountain)\n",
    "edited_image = pipe_edit(\n",
    "    prompt=prompt, image=image, num_inference_steps=20\n",
    ").images[0]\n",
    "\n",
    "make_image_grid([image, edited_image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-depth-sdxl-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "\n",
    "controlnet_pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "controlnet_pipeline.enable_model_cpu_offload()  # Optional, saves VRAM\n",
    "controlnet_pipeline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from controlnet_aux import MidasDetector\n",
    "from PIL import Image\n",
    "\n",
    "original_image = load_image(SampleURL.WomanSpeaking)\n",
    "original_image = original_image.resize((1024, 1024))\n",
    "\n",
    "# loads the MiDAS depth detector model\n",
    "midas = MidasDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
    "\n",
    "# Apply MiDAS depth detection\n",
    "processed_image_midas = midas(original_image).resize(\n",
    "    (1024, 1024), Image.BICUBIC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = controlnet_pipeline(\n",
    "    \"A colorful, ultra-realistic masked super hero singing a song\",\n",
    "    image=processed_image_midas,\n",
    "    controlnet_conditioning_scale=0.4,\n",
    "    num_inference_steps=30,\n",
    ").images[0]\n",
    "make_image_grid([original_image, processed_image_midas, image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Prompting and Image Variations\n",
    "\n",
    "### Image Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "sdxl_base_pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "sdxl_base_pipeline.to(device)\n",
    "\n",
    "# We load the IP Adapter too\n",
    "sdxl_base_pipeline.load_ip_adapter(\n",
    "    \"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\"\n",
    ")\n",
    "\n",
    "# We can set the scale of how strong we\n",
    "# want our IP Adapter to impact our overall result\n",
    "sdxl_base_pipeline.set_ip_adapter_scale(0.8)\n",
    "\n",
    "image = load_image(SampleURL.ItemsVariation)\n",
    "original_image = image.resize((1024, 1024))\n",
    "\n",
    "# Create the image variation\n",
    "generator = torch.Generator(device=device).manual_seed(1)\n",
    "variation_image = sdxl_base_pipeline(\n",
    "    prompt=\"\",\n",
    "    ip_adapter_image=original_image,\n",
    "    num_inference_steps=25,\n",
    "    generator=generator,\n",
    ").images\n",
    "\n",
    "make_image_grid([original_image, variation_image[0]], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Prompting\n",
    "\n",
    "#### Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the model and the IP Adapter, just as before\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Load the IP Adapter into the model\n",
    "pipeline.load_ip_adapter(\n",
    "    \"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\"\n",
    ")\n",
    "\n",
    "# We are applying the IP Adapter only to the mid block,\n",
    "# which is where it should be mapped to the style in SDXL\n",
    "scale = {\"up\": {\"block_0\": [0.0, 1.0, 0.0]}}\n",
    "pipeline.set_ip_adapter_scale(scale)\n",
    "\n",
    "image = load_image(SampleURL.Mamoeiro)\n",
    "original_image = image.resize((1024, 1024))\n",
    "\n",
    "# Run inference to generate the stylized image\n",
    "generator = torch.Generator(device=device).manual_seed(0)\n",
    "variation_image = pipeline(\n",
    "    prompt=\"a cat inside of a box\",\n",
    "    ip_adapter_image=original_image,\n",
    "    num_inference_steps=25,\n",
    "    generator=generator,\n",
    ").images\n",
    "\n",
    "make_image_grid([original_image, variation_image[0]], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Controls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the ControlNet pipeline\n",
    "controlnet_pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "controlnet_pipeline.to(device)\n",
    "\n",
    "# Load the IP Adapter\n",
    "controlnet_pipeline.load_ip_adapter(\n",
    "    \"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\"\n",
    ")\n",
    "# We are applying the IP Adapter only to the mid block,\n",
    "# which is where it should be mapped to the style in SDXL\n",
    "scale = {\n",
    "    \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\n",
    "}\n",
    "controlnet_pipeline.set_ip_adapter_scale(scale)\n",
    "\n",
    "# Load the original image\n",
    "original_image = load_image(SampleURL.WomanSpeaking)\n",
    "original_image = original_image.resize((1024, 1024))\n",
    "\n",
    "# Load the style image\n",
    "style_image = load_image(SampleURL.Mamoeiro)\n",
    "style_image = style_image.resize((1024, 1024))\n",
    "\n",
    "# Apply the MiDAS depth estimation\n",
    "processed_image_midas = midas(original_image).resize(\n",
    "    (1024, 1024), Image.BICUBIC\n",
    ")\n",
    "\n",
    "image = controlnet_pipeline(\n",
    "    \"A masked super hero singing a song\",\n",
    "    image=processed_image_midas,\n",
    "    ip_adapter_image=style_image,\n",
    "    controlnet_conditioning_scale=0.5,\n",
    ").images[0]\n",
    "make_image_grid(\n",
    "    [original_image, style_image, processed_image_midas, image], rows=1, cols=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "A big part of learning is putting your knowledge into practice. We strongly suggest not looking at the answers before taking a serious stab at it. Scroll down for the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**1. Explain how inpainting differs from image-to-image transformation and provide an example of a practical application.**\n",
    "\n",
    "Inpainting fills in or replaces specific masked areas of an image, while image-to-image transforms the entire image. Inpainting can be used to remove unwanted objects from images, such as removing a person from a photo. An example of image-to-image is converting a daytime scene into a nighttime scene.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How can prompt weighting help overcome the limitations of the diffusion models?**\n",
    "\n",
    "By providing fine-grained control over the prompt interpretation. \n",
    " \n",
    "Users can emphasize or de-emphasize certain elements in the final image by assigning different weights to various parts of the prompt. This is particularly useful when the model might be biased towards certain interpretations or when specific aspects of the prompt need more attention. For example, if a user wants to generate an image of \"a red car in a forest\" but the model keeps focusing too much on the forest, they could increase the weight of \"red car\" in the prompt. This fine-grained control helps users achieve their desired results more accurately, especially in cases where the default behavior of the model might not produce the intended outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What are the key differences between Prompt-to-Prompt editing and SEGA?**\n",
    "\n",
    "Prompt-to-Prompt \n",
    "* Focuses on modifying generated images by tweaking the input prompts while preserving the structure or overall composition of the original image.\n",
    "* Directly modifies cross-attention layers and leverages text embeddings to guide the editing process.\n",
    "\n",
    "SEGA\n",
    "* Aims to guide the generation process by amplifying or suppressing specific semantic concepts.\n",
    "* Adjusts the latent space guidance signal to prioritize semantic features during generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. How does ControlNet enhance the capabilities of diffusion models? Give examples of conditions that can be used with ControlNet.**\n",
    "\n",
    "ControlNet enhances the capabilities of diffusion models by allowing for additional control over the image generation process using various types of input conditions. It does this by training a separate neural network that learns to interpret these conditions and guide the main diffusion model accordingly. This allows for much more precise control over the generated images, beyond what's possible with text prompts alone. Some examples of conditions that can be used with ControlNet include:\n",
    "\n",
    "a. Edge maps or sketches, allowing the model to generate images that follow specific outlines or structures.\n",
    "b. Pose estimation data, which can guide the model to generate images of people or animals in specific poses.\n",
    "c. Depth maps, enabling the model to generate images with specific 3D structures or perspectives.\n",
    "d. Segmentation maps, allowing for control over the layout and composition of generated images.\n",
    "e. Normal maps, which can guide the model in generating images with specific surface details and textures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What is \"Inversion\" in the context of text-to-image models, and what does it allow us to do?**\n",
    "\n",
    "It's a process to map real images into the latent space of the model for editing. The real image is \"inverted\" back into noise mapped into the latent space which allows for perfect reconstruction and more excitingly, for editing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaibook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
